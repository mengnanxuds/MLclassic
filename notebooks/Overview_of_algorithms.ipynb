{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of Classic Machine Learning Algorithms\n",
    "\n",
    "## Purpose\n",
    "This notebook provides a high-level overview of popular classic machine learning algorithms, including:\n",
    "- Short theoretical explanations\n",
    "- Simple code examples using sample datasets\n",
    "- Visualizations of results\n",
    "\n",
    "It is designed for beginners to understand the principles and applications of these algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Linear Regression\n",
    "\n",
    "### Theory\n",
    "Linear Regression is used for predicting a continuous dependent variable based on one or more independent variables.  \n",
    "The relationship is modeled as:  \n",
    "\\[\n",
    "y = \\beta_0 + \\beta_1 x + \\epsilon\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### Code Example\n",
    "```python\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Generate sample data\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "\n",
    "# Train Linear Regression Model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Plot Results\n",
    "plt.scatter(X, y, label=\"Data\", color=\"blue\")\n",
    "plt.plot(X, y_pred, label=\"Regression Line\", color=\"red\")\n",
    "plt.title(\"Linear Regression Example\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Target\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "## 2. Decision Tree Classifier\n",
    "\n",
    "### Theory\n",
    "\n",
    "Decision Trees are non-parametric algorithms that split data recursively based on feature values.\n",
    "Used for both classification and regression tasks.\n",
    "\n",
    "\n",
    "### Code Example\n",
    "```python\n",
    "# Import libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Train Decision Tree\n",
    "clf = DecisionTreeClassifier(max_depth=3)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Plot the Decision Tree\n",
    "plt.figure(figsize=(10, 8))\n",
    "plot_tree(clf, feature_names=data.feature_names, class_names=data.target_names, filled=True)\n",
    "plt.title(\"Decision Tree Example\")\n",
    "plt.show()\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "## 3. K-Means Clustering\n",
    "\n",
    "### Theory\n",
    "\n",
    "K-Means is an unsupervised learning algorithm for clustering data into K groups based on their features.\n",
    "It minimizes the intra-cluster distances.\n",
    "\n",
    "\n",
    "\n",
    "### Code Example\n",
    "```python\n",
    "# Import libraries\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generate synthetic data\n",
    "X, _ = make_blobs(n_samples=300, centers=3, random_state=42)\n",
    "\n",
    "# Apply KMeans\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Plot Results\n",
    "plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='viridis', s=50)\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='x', s=200, label='Centroids')\n",
    "plt.title(\"K-Means Clustering Example\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "```\n",
    "## 4. Principal Component Analysis (PCA)\n",
    "\n",
    "### Theory\n",
    "\n",
    "PCA is a dimensionality reduction technique that projects data into fewer dimensions while preserving variance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Code Example\n",
    "```python\n",
    "\n",
    "# Import libraries\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Digits dataset\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Plot Results\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=digits.target, cmap='Spectral', s=20)\n",
    "plt.title(\"PCA Example on Digits Dataset\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.colorbar(label=\"Digit Class\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Algorithm             | Type                     | Example Library          | Key Features                   |\n",
    "|-----------------------|--------------------------|--------------------------|--------------------------------|\n",
    "| Linear Regression     | Regression              | `LinearRegression`       | Predict continuous outcomes    |\n",
    "| Decision Tree         | Classification          | `DecisionTreeClassifier` | Recursive data splitting       |\n",
    "| K-Means Clustering    | Clustering              | `KMeans`                 | Partition data into K clusters |\n",
    "| PCA                   | Dimensionality Reduction| `PCA`                    | Reduce feature dimensions      |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
